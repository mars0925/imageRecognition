Demo003

C:\ProgramData\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
x_train_image: (3200, 1, 32, 32)
x_train_label: (3200,)
x_TEST_image: (800, 1, 32, 32)
x_TEST_label: (800,)
訓練集張數 3200
測試集張數 800
圖片的像素 32
3200 train samples
800 test samples

標籤  2   2000
標籤  3   2000
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_1 (Dense)              (None, 128)               131200    
_________________________________________________________________
dropout_1 (Dropout)          (None, 128)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 128)               16512     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 10)                1290      
=================================================================
Total params: 149,002
Trainable params: 149,002
Non-trainable params: 0
_________________________________________________________________
Train on 3200 samples, validate on 800 samples
Epoch 1/20
2018-11-08 17:34:39.625365: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2

  64/3200 [..............................] - ETA: 8s - loss: 2.1621 - acc: 0.2031
1280/3200 [===========>..................] - ETA: 0s - loss: 0.8337 - acc: 0.4875
2496/3200 [======================>.......] - ETA: 0s - loss: 0.7665 - acc: 0.5196
3200/3200 [==============================] - 0s 106us/step - loss: 0.7506 - acc: 0.5225 - val_loss: 0.6932 - val_acc: 0.5012
Epoch 2/20

  64/3200 [..............................] - ETA: 0s - loss: 0.6943 - acc: 0.5000
1600/3200 [==============>...............] - ETA: 0s - loss: 0.7044 - acc: 0.5225
3136/3200 [============================>.] - ETA: 0s - loss: 0.6990 - acc: 0.5274
3200/3200 [==============================] - 0s 36us/step - loss: 0.6989 - acc: 0.5291 - val_loss: 0.6929 - val_acc: 0.5962
Epoch 3/20

  64/3200 [..............................] - ETA: 0s - loss: 0.6770 - acc: 0.5156
1536/3200 [=============>................] - ETA: 0s - loss: 0.6973 - acc: 0.5384
3072/3200 [===========================>..] - ETA: 0s - loss: 0.6971 - acc: 0.5410
3200/3200 [==============================] - 0s 36us/step - loss: 0.6970 - acc: 0.5428 - val_loss: 0.6924 - val_acc: 0.5062
Epoch 4/20

  64/3200 [..............................] - ETA: 0s - loss: 0.6894 - acc: 0.4531
1600/3200 [==============>...............] - ETA: 0s - loss: 0.6999 - acc: 0.5437
3136/3200 [============================>.] - ETA: 0s - loss: 0.6933 - acc: 0.5682
3200/3200 [==============================] - 0s 36us/step - loss: 0.6931 - acc: 0.5697 - val_loss: 0.6399 - val_acc: 0.5425
Epoch 5/20

  64/3200 [..............................] - ETA: 0s - loss: 0.6520 - acc: 0.5625
1600/3200 [==============>...............] - ETA: 0s - loss: 0.6826 - acc: 0.6131
3136/3200 [============================>.] - ETA: 0s - loss: 0.6606 - acc: 0.6189
3200/3200 [==============================] - 0s 36us/step - loss: 0.6616 - acc: 0.6181 - val_loss: 0.5938 - val_acc: 0.6925
Epoch 6/20

  64/3200 [..............................] - ETA: 0s - loss: 0.5876 - acc: 0.7031
1600/3200 [==============>...............] - ETA: 0s - loss: 0.6018 - acc: 0.6950
3136/3200 [============================>.] - ETA: 0s - loss: 0.5971 - acc: 0.6974
3200/3200 [==============================] - 0s 36us/step - loss: 0.5983 - acc: 0.6969 - val_loss: 0.5527 - val_acc: 0.7300
Epoch 7/20

  64/3200 [..............................] - ETA: 0s - loss: 0.6505 - acc: 0.6250
1536/3200 [=============>................] - ETA: 0s - loss: 0.5933 - acc: 0.7083
3072/3200 [===========================>..] - ETA: 0s - loss: 0.5823 - acc: 0.7142
3200/3200 [==============================] - 0s 37us/step - loss: 0.5886 - acc: 0.7097 - val_loss: 0.5518 - val_acc: 0.7238
Epoch 8/20

  64/3200 [..............................] - ETA: 0s - loss: 0.6316 - acc: 0.6250
1600/3200 [==============>...............] - ETA: 0s - loss: 0.5741 - acc: 0.7150
3136/3200 [============================>.] - ETA: 0s - loss: 0.5687 - acc: 0.7136
3200/3200 [==============================] - 0s 36us/step - loss: 0.5675 - acc: 0.7150 - val_loss: 0.5341 - val_acc: 0.7288
Epoch 9/20

  64/3200 [..............................] - ETA: 0s - loss: 0.6134 - acc: 0.6719
1600/3200 [==============>...............] - ETA: 0s - loss: 0.5655 - acc: 0.7125
3072/3200 [===========================>..] - ETA: 0s - loss: 0.5579 - acc: 0.7207
3200/3200 [==============================] - 0s 36us/step - loss: 0.5588 - acc: 0.7197 - val_loss: 0.5353 - val_acc: 0.7300
Epoch 10/20

  64/3200 [..............................] - ETA: 0s - loss: 0.4958 - acc: 0.7969
1600/3200 [==============>...............] - ETA: 0s - loss: 0.5581 - acc: 0.7150
3136/3200 [============================>.] - ETA: 0s - loss: 0.5533 - acc: 0.7168
3200/3200 [==============================] - 0s 36us/step - loss: 0.5534 - acc: 0.7166 - val_loss: 0.5570 - val_acc: 0.7175
Epoch 11/20

  64/3200 [..............................] - ETA: 0s - loss: 0.5099 - acc: 0.6406
1536/3200 [=============>................] - ETA: 0s - loss: 0.5413 - acc: 0.7383
3072/3200 [===========================>..] - ETA: 0s - loss: 0.5441 - acc: 0.7327
3200/3200 [==============================] - 0s 37us/step - loss: 0.5470 - acc: 0.7313 - val_loss: 0.5515 - val_acc: 0.7262
Epoch 12/20

  64/3200 [..............................] - ETA: 0s - loss: 0.4807 - acc: 0.7656
1536/3200 [=============>................] - ETA: 0s - loss: 0.5403 - acc: 0.7331
3072/3200 [===========================>..] - ETA: 0s - loss: 0.5438 - acc: 0.7347
3200/3200 [==============================] - 0s 36us/step - loss: 0.5443 - acc: 0.7334 - val_loss: 0.5220 - val_acc: 0.7400
Epoch 13/20

  64/3200 [..............................] - ETA: 0s - loss: 0.5028 - acc: 0.7500
1600/3200 [==============>...............] - ETA: 0s - loss: 0.5256 - acc: 0.7431
3136/3200 [============================>.] - ETA: 0s - loss: 0.5360 - acc: 0.7398
3200/3200 [==============================] - 0s 36us/step - loss: 0.5350 - acc: 0.7397 - val_loss: 0.5229 - val_acc: 0.7388
Epoch 14/20

  64/3200 [..............................] - ETA: 0s - loss: 0.4969 - acc: 0.8125
1600/3200 [==============>...............] - ETA: 0s - loss: 0.5431 - acc: 0.7431
3136/3200 [============================>.] - ETA: 0s - loss: 0.5407 - acc: 0.7353
3200/3200 [==============================] - 0s 36us/step - loss: 0.5421 - acc: 0.7356 - val_loss: 0.5225 - val_acc: 0.7450
Epoch 15/20

  64/3200 [..............................] - ETA: 0s - loss: 0.3981 - acc: 0.8281
1600/3200 [==============>...............] - ETA: 0s - loss: 0.5287 - acc: 0.7462
3072/3200 [===========================>..] - ETA: 0s - loss: 0.5418 - acc: 0.7308
3200/3200 [==============================] - 0s 37us/step - loss: 0.5416 - acc: 0.7297 - val_loss: 0.5236 - val_acc: 0.7400
Epoch 16/20

  64/3200 [..............................] - ETA: 0s - loss: 0.5030 - acc: 0.7500
1600/3200 [==============>...............] - ETA: 0s - loss: 0.5194 - acc: 0.7456
3008/3200 [===========================>..] - ETA: 0s - loss: 0.5316 - acc: 0.7390
3200/3200 [==============================] - 0s 37us/step - loss: 0.5344 - acc: 0.7381 - val_loss: 0.5190 - val_acc: 0.7400
Epoch 17/20

  64/3200 [..............................] - ETA: 0s - loss: 0.5256 - acc: 0.7656
1472/3200 [============>.................] - ETA: 0s - loss: 0.5266 - acc: 0.7520
2944/3200 [==========================>...] - ETA: 0s - loss: 0.5229 - acc: 0.7534
3200/3200 [==============================] - 0s 38us/step - loss: 0.5272 - acc: 0.7484 - val_loss: 0.5332 - val_acc: 0.7550
Epoch 18/20

  64/3200 [..............................] - ETA: 0s - loss: 0.5951 - acc: 0.6875
1536/3200 [=============>................] - ETA: 0s - loss: 0.5330 - acc: 0.7441
3008/3200 [===========================>..] - ETA: 0s - loss: 0.5274 - acc: 0.7420
3200/3200 [==============================] - 0s 38us/step - loss: 0.5282 - acc: 0.7422 - val_loss: 0.5217 - val_acc: 0.7450
Epoch 19/20

  64/3200 [..............................] - ETA: 0s - loss: 0.5688 - acc: 0.7188
1600/3200 [==============>...............] - ETA: 0s - loss: 0.5050 - acc: 0.7606
3136/3200 [============================>.] - ETA: 0s - loss: 0.5197 - acc: 0.7484
3200/3200 [==============================] - 0s 36us/step - loss: 0.5214 - acc: 0.7478 - val_loss: 0.5327 - val_acc: 0.7388
Epoch 20/20

  64/3200 [..............................] - ETA: 0s - loss: 0.4984 - acc: 0.7188
1536/3200 [=============>................] - ETA: 0s - loss: 0.5166 - acc: 0.7546
3072/3200 [===========================>..] - ETA: 0s - loss: 0.5181 - acc: 0.7490
3200/3200 [==============================] - 0s 37us/step - loss: 0.5203 - acc: 0.7484 - val_loss: 0.5507 - val_acc: 0.7175
Test loss: 0.5507294714450837
Test accuracy: 0.7175
success
[Finished in 37.5s]

==============================================================================================================================================
=======================================================================================================================================
Demo004

C:\ProgramData\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
標籤  0   2000
標籤  1   2000

model.add(Dense(num_classes, activation='sigmoid'))

x_train_image: (3200, 1, 32, 32)
x_train_label: (3200,)
x_TEST_image: (800, 1, 32, 32)
x_TEST_label: (800,)
訓練集張數 3200
測試集張數 800
圖片的像素 32
3200 train samples
800 test samples
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_1 (Dense)              (None, 128)               131200    
_________________________________________________________________
dropout_1 (Dropout)          (None, 128)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 128)               16512     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 2)                 258       
=================================================================
Total params: 147,970
Trainable params: 147,970
Non-trainable params: 0
_________________________________________________________________
Train on 3200 samples, validate on 800 samples
Epoch 1/20
2018-11-08 17:48:20.449459: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2

  64/3200 [..............................] - ETA: 8s - loss: 0.6906 - acc: 0.5938
1280/3200 [===========>..................] - ETA: 0s - loss: 0.7248 - acc: 0.5023
2112/3200 [==================>...........] - ETA: 0s - loss: 0.7077 - acc: 0.5028
3200/3200 [==============================] - 0s 109us/step - loss: 0.7011 - acc: 0.5250 - val_loss: 0.6789 - val_acc: 0.6800
Epoch 2/20

  64/3200 [..............................] - ETA: 0s - loss: 0.6750 - acc: 0.5625
1344/3200 [===========>..................] - ETA: 0s - loss: 0.6731 - acc: 0.6101
2688/3200 [========================>.....] - ETA: 0s - loss: 0.6520 - acc: 0.6283
3200/3200 [==============================] - 0s 41us/step - loss: 0.6475 - acc: 0.6384 - val_loss: 0.5741 - val_acc: 0.7000
Epoch 3/20

  64/3200 [..............................] - ETA: 0s - loss: 0.4958 - acc: 0.7656
1344/3200 [===========>..................] - ETA: 0s - loss: 0.6097 - acc: 0.6815
2560/3200 [=======================>......] - ETA: 0s - loss: 0.6082 - acc: 0.6801
3200/3200 [==============================] - 0s 43us/step - loss: 0.6045 - acc: 0.6784 - val_loss: 0.5401 - val_acc: 0.7388
Epoch 4/20

  64/3200 [..............................] - ETA: 0s - loss: 0.4708 - acc: 0.7812
1152/3200 [=========>....................] - ETA: 0s - loss: 0.5699 - acc: 0.6988
2368/3200 [=====================>........] - ETA: 0s - loss: 0.5720 - acc: 0.7073
3200/3200 [==============================] - 0s 46us/step - loss: 0.5800 - acc: 0.7041 - val_loss: 0.6030 - val_acc: 0.6725
Epoch 5/20

  64/3200 [..............................] - ETA: 0s - loss: 0.6300 - acc: 0.6719
1472/3200 [============>.................] - ETA: 0s - loss: 0.5617 - acc: 0.7133
2880/3200 [==========================>...] - ETA: 0s - loss: 0.5686 - acc: 0.7149
3200/3200 [==============================] - 0s 40us/step - loss: 0.5687 - acc: 0.7150 - val_loss: 0.5373 - val_acc: 0.7375
Epoch 6/20

  64/3200 [..............................] - ETA: 0s - loss: 0.5441 - acc: 0.7344
1408/3200 [============>.................] - ETA: 0s - loss: 0.5587 - acc: 0.7287
2816/3200 [=========================>....] - ETA: 0s - loss: 0.5639 - acc: 0.7216
3200/3200 [==============================] - 0s 40us/step - loss: 0.5661 - acc: 0.7169 - val_loss: 0.5204 - val_acc: 0.7562
Epoch 7/20

  64/3200 [..............................] - ETA: 0s - loss: 0.4913 - acc: 0.7812
1408/3200 [============>.................] - ETA: 0s - loss: 0.5670 - acc: 0.7095
2816/3200 [=========================>....] - ETA: 0s - loss: 0.5531 - acc: 0.7202
3200/3200 [==============================] - 0s 40us/step - loss: 0.5528 - acc: 0.7197 - val_loss: 0.5966 - val_acc: 0.6687
Epoch 8/20

  64/3200 [..............................] - ETA: 0s - loss: 0.6604 - acc: 0.6875
1472/3200 [============>.................] - ETA: 0s - loss: 0.5436 - acc: 0.7398
2880/3200 [==========================>...] - ETA: 0s - loss: 0.5624 - acc: 0.7198
3200/3200 [==============================] - 0s 40us/step - loss: 0.5619 - acc: 0.7200 - val_loss: 0.5151 - val_acc: 0.7625
Epoch 9/20

  64/3200 [..............................] - ETA: 0s - loss: 0.4493 - acc: 0.7969
1472/3200 [============>.................] - ETA: 0s - loss: 0.5285 - acc: 0.7452
2880/3200 [==========================>...] - ETA: 0s - loss: 0.5422 - acc: 0.7319
3200/3200 [==============================] - 0s 39us/step - loss: 0.5436 - acc: 0.7300 - val_loss: 0.5304 - val_acc: 0.7550
Epoch 10/20

  64/3200 [..............................] - ETA: 0s - loss: 0.5151 - acc: 0.7031
1472/3200 [============>.................] - ETA: 0s - loss: 0.5358 - acc: 0.7283
2880/3200 [==========================>...] - ETA: 0s - loss: 0.5504 - acc: 0.7208
3200/3200 [==============================] - 0s 39us/step - loss: 0.5522 - acc: 0.7203 - val_loss: 0.5284 - val_acc: 0.7488
Epoch 11/20

  64/3200 [..............................] - ETA: 0s - loss: 0.6028 - acc: 0.6562
1472/3200 [============>.................] - ETA: 0s - loss: 0.5447 - acc: 0.7201
2880/3200 [==========================>...] - ETA: 0s - loss: 0.5414 - acc: 0.7250
3200/3200 [==============================] - 0s 39us/step - loss: 0.5410 - acc: 0.7253 - val_loss: 0.5080 - val_acc: 0.7562
Epoch 12/20

  64/3200 [..............................] - ETA: 0s - loss: 0.4019 - acc: 0.8438
1472/3200 [============>.................] - ETA: 0s - loss: 0.5319 - acc: 0.7500
2880/3200 [==========================>...] - ETA: 0s - loss: 0.5420 - acc: 0.7316
3200/3200 [==============================] - 0s 40us/step - loss: 0.5430 - acc: 0.7328 - val_loss: 0.5419 - val_acc: 0.7262
Epoch 13/20

  64/3200 [..............................] - ETA: 0s - loss: 0.5486 - acc: 0.6719
1472/3200 [============>.................] - ETA: 0s - loss: 0.5329 - acc: 0.7452
2816/3200 [=========================>....] - ETA: 0s - loss: 0.5336 - acc: 0.7386
3200/3200 [==============================] - 0s 40us/step - loss: 0.5372 - acc: 0.7366 - val_loss: 0.5257 - val_acc: 0.7400
Epoch 14/20

  64/3200 [..............................] - ETA: 0s - loss: 0.6147 - acc: 0.6875
1408/3200 [============>.................] - ETA: 0s - loss: 0.5543 - acc: 0.7237
2816/3200 [=========================>....] - ETA: 0s - loss: 0.5397 - acc: 0.7312
3200/3200 [==============================] - 0s 41us/step - loss: 0.5390 - acc: 0.7306 - val_loss: 0.5096 - val_acc: 0.7612
Epoch 15/20

  64/3200 [..............................] - ETA: 0s - loss: 0.4865 - acc: 0.7656
1408/3200 [============>.................] - ETA: 0s - loss: 0.5466 - acc: 0.7322
2816/3200 [=========================>....] - ETA: 0s - loss: 0.5296 - acc: 0.7440
3200/3200 [==============================] - 0s 41us/step - loss: 0.5291 - acc: 0.7463 - val_loss: 0.5187 - val_acc: 0.7538
Epoch 16/20

  64/3200 [..............................] - ETA: 0s - loss: 0.5779 - acc: 0.7344
1280/3200 [===========>..................] - ETA: 0s - loss: 0.5137 - acc: 0.7484
2496/3200 [======================>.......] - ETA: 0s - loss: 0.5237 - acc: 0.7428
3200/3200 [==============================] - 0s 44us/step - loss: 0.5291 - acc: 0.7428 - val_loss: 0.5292 - val_acc: 0.7388
Epoch 17/20

  64/3200 [..............................] - ETA: 0s - loss: 0.5136 - acc: 0.7188
1472/3200 [============>.................] - ETA: 0s - loss: 0.5478 - acc: 0.7296
2816/3200 [=========================>....] - ETA: 0s - loss: 0.5387 - acc: 0.7344
3200/3200 [==============================] - 0s 42us/step - loss: 0.5321 - acc: 0.7378 - val_loss: 0.5069 - val_acc: 0.7550
Epoch 18/20

  64/3200 [..............................] - ETA: 0s - loss: 0.5064 - acc: 0.7656
1216/3200 [==========>...................] - ETA: 0s - loss: 0.5262 - acc: 0.7303
2496/3200 [======================>.......] - ETA: 0s - loss: 0.5323 - acc: 0.7328
3200/3200 [==============================] - 0s 66us/step - loss: 0.5335 - acc: 0.7344 - val_loss: 0.5055 - val_acc: 0.7675
Epoch 19/20

  64/3200 [..............................] - ETA: 0s - loss: 0.3916 - acc: 0.8125
1088/3200 [=========>....................] - ETA: 0s - loss: 0.5303 - acc: 0.7316
2112/3200 [==================>...........] - ETA: 0s - loss: 0.5306 - acc: 0.7358
3072/3200 [===========================>..] - ETA: 0s - loss: 0.5263 - acc: 0.7412
3200/3200 [==============================] - 0s 55us/step - loss: 0.5243 - acc: 0.7434 - val_loss: 0.5011 - val_acc: 0.7662
Epoch 20/20

  64/3200 [..............................] - ETA: 0s - loss: 0.4315 - acc: 0.7656
1216/3200 [==========>...................] - ETA: 0s - loss: 0.5041 - acc: 0.7459
2368/3200 [=====================>........] - ETA: 0s - loss: 0.5270 - acc: 0.7361
3200/3200 [==============================] - 0s 46us/step - loss: 0.5266 - acc: 0.7381 - val_loss: 0.5053 - val_acc: 0.7688
Test loss: 0.5053014540672303
Test accuracy: 0.76875


===============================================================================================================================================================================

demo005
原標籤2跟3

C:\ProgramData\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
標籤  0   2468
標籤  1   2500
x_train_image: (3974, 1, 32, 32)
x_train_label: (3974,)
x_TEST_image: (994, 1, 32, 32)
x_TEST_label: (994,)
訓練集張數 3974
測試集張數 994
圖片的像素 32
3974 train samples
994 test samples
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_1 (Dense)              (None, 128)               131200    
_________________________________________________________________
dropout_1 (Dropout)          (None, 128)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 128)               16512     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 2)                 258       
=================================================================
Total params: 147,970
Trainable params: 147,970
Non-trainable params: 0
_________________________________________________________________
Train on 3974 samples, validate on 994 samples
Epoch 1/20
2018-11-08 17:59:16.266087: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2

  64/3974 [..............................] - ETA: 10s - loss: 0.7206 - acc: 0.6250
1216/3974 [========>.....................] - ETA: 0s - loss: 0.7036 - acc: 0.5329 
2368/3974 [================>.............] - ETA: 0s - loss: 0.6988 - acc: 0.5300
3712/3974 [===========================>..] - ETA: 0s - loss: 0.6918 - acc: 0.5509
3974/3974 [==============================] - 0s 94us/step - loss: 0.6917 - acc: 0.5506 - val_loss: 0.7138 - val_acc: 0.4789
Epoch 2/20

  64/3974 [..............................] - ETA: 0s - loss: 0.5980 - acc: 0.5938
1472/3974 [==========>...................] - ETA: 0s - loss: 0.6729 - acc: 0.5904
2944/3974 [=====================>........] - ETA: 0s - loss: 0.6629 - acc: 0.6087
3974/3974 [==============================] - 0s 39us/step - loss: 0.6561 - acc: 0.6183 - val_loss: 0.6389 - val_acc: 0.6579
Epoch 3/20

  64/3974 [..............................] - ETA: 0s - loss: 0.6283 - acc: 0.6406
1472/3974 [==========>...................] - ETA: 0s - loss: 0.6318 - acc: 0.6590
2880/3974 [====================>.........] - ETA: 0s - loss: 0.6357 - acc: 0.6500
3974/3974 [==============================] - 0s 39us/step - loss: 0.6374 - acc: 0.6467 - val_loss: 0.7065 - val_acc: 0.6087
Epoch 4/20

  64/3974 [..............................] - ETA: 0s - loss: 0.6155 - acc: 0.6719
1536/3974 [==========>...................] - ETA: 0s - loss: 0.6248 - acc: 0.6680
3008/3974 [=====================>........] - ETA: 0s - loss: 0.6247 - acc: 0.6646
3974/3974 [==============================] - 0s 38us/step - loss: 0.6237 - acc: 0.6653 - val_loss: 0.6861 - val_acc: 0.6207
Epoch 5/20

  64/3974 [..............................] - ETA: 0s - loss: 0.6687 - acc: 0.6719
1536/3974 [==========>...................] - ETA: 0s - loss: 0.6251 - acc: 0.6693
3008/3974 [=====================>........] - ETA: 0s - loss: 0.6279 - acc: 0.6596
3974/3974 [==============================] - 0s 37us/step - loss: 0.6275 - acc: 0.6636 - val_loss: 0.6687 - val_acc: 0.5795
Epoch 6/20

  64/3974 [..............................] - ETA: 0s - loss: 0.6681 - acc: 0.5938
1536/3974 [==========>...................] - ETA: 0s - loss: 0.6385 - acc: 0.6504
3008/3974 [=====================>........] - ETA: 0s - loss: 0.6241 - acc: 0.6626
3974/3974 [==============================] - 0s 38us/step - loss: 0.6190 - acc: 0.6681 - val_loss: 0.6861 - val_acc: 0.6046
Epoch 7/20

  64/3974 [..............................] - ETA: 0s - loss: 0.6279 - acc: 0.6250
1536/3974 [==========>...................] - ETA: 0s - loss: 0.6163 - acc: 0.6569
2944/3974 [=====================>........] - ETA: 0s - loss: 0.6200 - acc: 0.6607
3974/3974 [==============================] - 0s 38us/step - loss: 0.6141 - acc: 0.6706 - val_loss: 0.6775 - val_acc: 0.6026
Epoch 8/20

  64/3974 [..............................] - ETA: 0s - loss: 0.7096 - acc: 0.5781
1536/3974 [==========>...................] - ETA: 0s - loss: 0.6089 - acc: 0.6842
3008/3974 [=====================>........] - ETA: 0s - loss: 0.6122 - acc: 0.6792
3974/3974 [==============================] - 0s 39us/step - loss: 0.6060 - acc: 0.6814 - val_loss: 0.6192 - val_acc: 0.6670
Epoch 9/20

  64/3974 [..............................] - ETA: 0s - loss: 0.6159 - acc: 0.6875
1472/3974 [==========>...................] - ETA: 0s - loss: 0.6039 - acc: 0.6868
2944/3974 [=====================>........] - ETA: 0s - loss: 0.6062 - acc: 0.6855
3974/3974 [==============================] - 0s 38us/step - loss: 0.6078 - acc: 0.6802 - val_loss: 0.6568 - val_acc: 0.6157
Epoch 10/20

  64/3974 [..............................] - ETA: 0s - loss: 0.5100 - acc: 0.7500
1472/3974 [==========>...................] - ETA: 0s - loss: 0.6025 - acc: 0.6957
2944/3974 [=====================>........] - ETA: 0s - loss: 0.6082 - acc: 0.6851
3974/3974 [==============================] - 0s 38us/step - loss: 0.6009 - acc: 0.6910 - val_loss: 1.0409 - val_acc: 0.5241
Epoch 11/20

  64/3974 [..............................] - ETA: 0s - loss: 1.1332 - acc: 0.5625
1472/3974 [==========>...................] - ETA: 0s - loss: 0.6306 - acc: 0.6732
2944/3974 [=====================>........] - ETA: 0s - loss: 0.6126 - acc: 0.6841
3974/3974 [==============================] - 0s 38us/step - loss: 0.6055 - acc: 0.6910 - val_loss: 0.6260 - val_acc: 0.6630
Epoch 12/20

  64/3974 [..............................] - ETA: 0s - loss: 0.5883 - acc: 0.7344
1536/3974 [==========>...................] - ETA: 0s - loss: 0.5987 - acc: 0.7025
3008/3974 [=====================>........] - ETA: 0s - loss: 0.5979 - acc: 0.6941
3974/3974 [==============================] - 0s 38us/step - loss: 0.5995 - acc: 0.6945 - val_loss: 0.6513 - val_acc: 0.6298
Epoch 13/20

  64/3974 [..............................] - ETA: 0s - loss: 0.6452 - acc: 0.6094
1472/3974 [==========>...................] - ETA: 0s - loss: 0.6085 - acc: 0.6760
2944/3974 [=====================>........] - ETA: 0s - loss: 0.5996 - acc: 0.6878
3974/3974 [==============================] - 0s 38us/step - loss: 0.5931 - acc: 0.6928 - val_loss: 0.6636 - val_acc: 0.6479
Epoch 14/20

  64/3974 [..............................] - ETA: 0s - loss: 0.5784 - acc: 0.6875
1536/3974 [==========>...................] - ETA: 0s - loss: 0.5866 - acc: 0.7083
3072/3974 [======================>.......] - ETA: 0s - loss: 0.5834 - acc: 0.7031
3974/3974 [==============================] - 0s 38us/step - loss: 0.5877 - acc: 0.7018 - val_loss: 0.6137 - val_acc: 0.6690
Epoch 15/20

  64/3974 [..............................] - ETA: 0s - loss: 0.5647 - acc: 0.7188
1472/3974 [==========>...................] - ETA: 0s - loss: 0.5702 - acc: 0.7024
2944/3974 [=====================>........] - ETA: 0s - loss: 0.5825 - acc: 0.6980
3974/3974 [==============================] - 0s 39us/step - loss: 0.5889 - acc: 0.6973 - val_loss: 0.6057 - val_acc: 0.6811
Epoch 16/20

  64/3974 [..............................] - ETA: 0s - loss: 0.5411 - acc: 0.8281
1536/3974 [==========>...................] - ETA: 0s - loss: 0.5942 - acc: 0.7018
3008/3974 [=====================>........] - ETA: 0s - loss: 0.5866 - acc: 0.6981
3974/3974 [==============================] - 0s 38us/step - loss: 0.5870 - acc: 0.7018 - val_loss: 0.5983 - val_acc: 0.6881
Epoch 17/20

  64/3974 [..............................] - ETA: 0s - loss: 0.5284 - acc: 0.7500
1536/3974 [==========>...................] - ETA: 0s - loss: 0.5927 - acc: 0.6914
3008/3974 [=====================>........] - ETA: 0s - loss: 0.5873 - acc: 0.7008
3974/3974 [==============================] - 0s 39us/step - loss: 0.5839 - acc: 0.7033 - val_loss: 0.6259 - val_acc: 0.6670
Epoch 18/20

  64/3974 [..............................] - ETA: 0s - loss: 0.5509 - acc: 0.6875
1536/3974 [==========>...................] - ETA: 0s - loss: 0.5717 - acc: 0.7240
3008/3974 [=====================>........] - ETA: 0s - loss: 0.5819 - acc: 0.7158
3974/3974 [==============================] - 0s 38us/step - loss: 0.5854 - acc: 0.7089 - val_loss: 0.6132 - val_acc: 0.6610
Epoch 19/20

  64/3974 [..............................] - ETA: 0s - loss: 0.5114 - acc: 0.7188
1536/3974 [==========>...................] - ETA: 0s - loss: 0.5855 - acc: 0.7025
3008/3974 [=====================>........] - ETA: 0s - loss: 0.5848 - acc: 0.7058
3974/3974 [==============================] - 0s 38us/step - loss: 0.5805 - acc: 0.7079 - val_loss: 0.6168 - val_acc: 0.6620
Epoch 20/20

  64/3974 [..............................] - ETA: 0s - loss: 0.6345 - acc: 0.7188
1472/3974 [==========>...................] - ETA: 0s - loss: 0.5579 - acc: 0.7317
2944/3974 [=====================>........] - ETA: 0s - loss: 0.5724 - acc: 0.7191
3974/3974 [==============================] - 0s 38us/step - loss: 0.5776 - acc: 0.7124 - val_loss: 0.5979 - val_acc: 0.6871
Test loss: 0.5978507231058969
Test accuracy: 0.6871227364185111



============================================================================================================================================================================================================================================================================

Demo006
沒有調整label

C:\ProgramData\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
標籤  2   2468
標籤  3   2500
x_train_image: (3974, 1, 32, 32)
x_train_label: (3974,)
x_TEST_image: (994, 1, 32, 32)
x_TEST_label: (994,)
訓練集張數 3974
測試集張數 994
圖片的像素 32
3974 train samples
994 test samples
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_1 (Dense)              (None, 128)               131200    
_________________________________________________________________
dropout_1 (Dropout)          (None, 128)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 128)               16512     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 10)                1290      
=================================================================
Total params: 149,002
Trainable params: 149,002
Non-trainable params: 0
_________________________________________________________________
Train on 3974 samples, validate on 994 samples
Epoch 1/20
2018-11-08 18:04:43.714874: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2

  64/3974 [..............................] - ETA: 10s - loss: 2.4467 - acc: 0.0312
1280/3974 [========>.....................] - ETA: 0s - loss: 0.8979 - acc: 0.4617 
2496/3974 [=================>............] - ETA: 0s - loss: 0.8001 - acc: 0.4712
3904/3974 [============================>.] - ETA: 0s - loss: 0.7617 - acc: 0.4785
3974/3974 [==============================] - 0s 94us/step - loss: 0.7605 - acc: 0.4774 - val_loss: 0.6932 - val_acc: 0.4779
Epoch 2/20

  64/3974 [..............................] - ETA: 0s - loss: 0.6928 - acc: 0.5156
1408/3974 [=========>....................] - ETA: 0s - loss: 0.6941 - acc: 0.4908
2752/3974 [===================>..........] - ETA: 0s - loss: 0.6956 - acc: 0.5025
3974/3974 [==============================] - 0s 41us/step - loss: 0.6950 - acc: 0.5030 - val_loss: 0.6931 - val_acc: 0.5050
Epoch 3/20

  64/3974 [..............................] - ETA: 0s - loss: 0.6937 - acc: 0.3750
1472/3974 [==========>...................] - ETA: 0s - loss: 0.6933 - acc: 0.4823
2816/3974 [====================>.........] - ETA: 0s - loss: 0.6932 - acc: 0.4840
3974/3974 [==============================] - 0s 41us/step - loss: 0.6932 - acc: 0.4819 - val_loss: 0.6931 - val_acc: 0.4789
Epoch 4/20

  64/3974 [..............................] - ETA: 0s - loss: 0.6932 - acc: 0.5000
1408/3974 [=========>....................] - ETA: 0s - loss: 0.6931 - acc: 0.4709
2816/3974 [====================>.........] - ETA: 0s - loss: 0.6948 - acc: 0.4666
3974/3974 [==============================] - 0s 41us/step - loss: 0.6943 - acc: 0.4668 - val_loss: 0.6931 - val_acc: 0.4225
Epoch 5/20

  64/3974 [..............................] - ETA: 0s - loss: 0.6932 - acc: 0.4531
1344/3974 [=========>....................] - ETA: 0s - loss: 0.6932 - acc: 0.4799
2688/3974 [===================>..........] - ETA: 0s - loss: 0.6931 - acc: 0.4721
3974/3974 [==============================] - 0s 42us/step - loss: 0.6931 - acc: 0.4776 - val_loss: 0.6931 - val_acc: 0.4960
Epoch 6/20

  64/3974 [..............................] - ETA: 0s - loss: 0.6931 - acc: 0.4375
1344/3974 [=========>....................] - ETA: 0s - loss: 0.6931 - acc: 0.4926
2688/3974 [===================>..........] - ETA: 0s - loss: 0.6931 - acc: 0.4877
3974/3974 [==============================] - 0s 42us/step - loss: 0.6932 - acc: 0.4889 - val_loss: 0.6931 - val_acc: 0.5050
Epoch 7/20

  64/3974 [..............................] - ETA: 0s - loss: 0.7064 - acc: 0.5469
1344/3974 [=========>....................] - ETA: 0s - loss: 0.6941 - acc: 0.4985
2688/3974 [===================>..........] - ETA: 0s - loss: 0.6939 - acc: 0.4914
3974/3974 [==============================] - 0s 41us/step - loss: 0.6936 - acc: 0.4902 - val_loss: 0.6931 - val_acc: 0.5050
Epoch 8/20

  64/3974 [..............................] - ETA: 0s - loss: 0.6932 - acc: 0.4375
1536/3974 [==========>...................] - ETA: 0s - loss: 0.6932 - acc: 0.4915
3008/3974 [=====================>........] - ETA: 0s - loss: 0.6932 - acc: 0.4910
3974/3974 [==============================] - 0s 38us/step - loss: 0.6932 - acc: 0.4854 - val_loss: 0.6931 - val_acc: 0.5050
Epoch 9/20

  64/3974 [..............................] - ETA: 0s - loss: 0.6931 - acc: 0.4375
1536/3974 [==========>...................] - ETA: 0s - loss: 0.6932 - acc: 0.5085
3008/3974 [=====================>........] - ETA: 0s - loss: 0.6931 - acc: 0.4940
3974/3974 [==============================] - 0s 39us/step - loss: 0.6931 - acc: 0.4917 - val_loss: 0.6931 - val_acc: 0.5050
Epoch 10/20

  64/3974 [..............................] - ETA: 0s - loss: 0.6931 - acc: 0.5625
1472/3974 [==========>...................] - ETA: 0s - loss: 0.6936 - acc: 0.5041
2944/3974 [=====================>........] - ETA: 0s - loss: 0.6937 - acc: 0.4993
3974/3974 [==============================] - 0s 39us/step - loss: 0.6936 - acc: 0.4932 - val_loss: 0.6931 - val_acc: 0.5050
Epoch 11/20

  64/3974 [..............................] - ETA: 0s - loss: 0.6932 - acc: 0.4531
1472/3974 [==========>...................] - ETA: 0s - loss: 0.6932 - acc: 0.4939
2944/3974 [=====================>........] - ETA: 0s - loss: 0.6932 - acc: 0.5082
3974/3974 [==============================] - 0s 38us/step - loss: 0.6932 - acc: 0.4950 - val_loss: 0.6931 - val_acc: 0.5050
Epoch 12/20

  64/3974 [..............................] - ETA: 0s - loss: 0.6931 - acc: 0.4062
1472/3974 [==========>...................] - ETA: 0s - loss: 0.6948 - acc: 0.4932
2944/3974 [=====================>........] - ETA: 0s - loss: 0.6940 - acc: 0.4878
3974/3974 [==============================] - 0s 39us/step - loss: 0.6938 - acc: 0.4947 - val_loss: 0.6931 - val_acc: 0.5050
Epoch 13/20

  64/3974 [..............................] - ETA: 0s - loss: 0.6931 - acc: 0.5156
1472/3974 [==========>...................] - ETA: 0s - loss: 0.6931 - acc: 0.5000
2880/3974 [====================>.........] - ETA: 0s - loss: 0.6932 - acc: 0.5003
3974/3974 [==============================] - 0s 39us/step - loss: 0.6939 - acc: 0.4960 - val_loss: 0.6931 - val_acc: 0.5050
Epoch 14/20

  64/3974 [..............................] - ETA: 0s - loss: 0.6929 - acc: 0.3594
1536/3974 [==========>...................] - ETA: 0s - loss: 0.6934 - acc: 0.4668
2944/3974 [=====================>........] - ETA: 0s - loss: 0.6933 - acc: 0.4949
3974/3974 [==============================] - 0s 38us/step - loss: 0.6933 - acc: 0.4955 - val_loss: 0.6931 - val_acc: 0.5050
Epoch 15/20

  64/3974 [..............................] - ETA: 0s - loss: 0.6931 - acc: 0.4531
1472/3974 [==========>...................] - ETA: 0s - loss: 0.6932 - acc: 0.4817
2944/3974 [=====================>........] - ETA: 0s - loss: 0.6932 - acc: 0.4912
3974/3974 [==============================] - 0s 38us/step - loss: 0.6933 - acc: 0.4935 - val_loss: 0.6931 - val_acc: 0.5050
Epoch 16/20

  64/3974 [..............................] - ETA: 0s - loss: 0.6929 - acc: 0.4688
1472/3974 [==========>...................] - ETA: 0s - loss: 0.6932 - acc: 0.4980
2880/3974 [====================>.........] - ETA: 0s - loss: 0.6938 - acc: 0.4990
3974/3974 [==============================] - 0s 39us/step - loss: 0.6937 - acc: 0.4942 - val_loss: 0.6931 - val_acc: 0.5050
Epoch 17/20

  64/3974 [..............................] - ETA: 0s - loss: 0.6932 - acc: 0.5000
1472/3974 [==========>...................] - ETA: 0s - loss: 0.6931 - acc: 0.4993
2944/3974 [=====================>........] - ETA: 0s - loss: 0.6932 - acc: 0.5017
3974/3974 [==============================] - 0s 39us/step - loss: 0.6932 - acc: 0.4952 - val_loss: 0.6931 - val_acc: 0.5050
Epoch 18/20

  64/3974 [..............................] - ETA: 0s - loss: 0.6931 - acc: 0.4531
1472/3974 [==========>...................] - ETA: 0s - loss: 0.6931 - acc: 0.4864
2880/3974 [====================>.........] - ETA: 0s - loss: 0.6932 - acc: 0.4934
3974/3974 [==============================] - 0s 39us/step - loss: 0.6932 - acc: 0.4955 - val_loss: 0.6931 - val_acc: 0.5060
Epoch 19/20

  64/3974 [..............................] - ETA: 0s - loss: 0.6931 - acc: 0.4844
1472/3974 [==========>...................] - ETA: 0s - loss: 0.6931 - acc: 0.4973
2944/3974 [=====================>........] - ETA: 0s - loss: 0.6931 - acc: 0.4939
3974/3974 [==============================] - 0s 39us/step - loss: 0.6931 - acc: 0.4930 - val_loss: 0.6931 - val_acc: 0.5050
Epoch 20/20

  64/3974 [..............................] - ETA: 0s - loss: 0.6931 - acc: 0.5625
1472/3974 [==========>...................] - ETA: 0s - loss: 0.6932 - acc: 0.4939
2944/3974 [=====================>........] - ETA: 0s - loss: 0.6931 - acc: 0.4949
3974/3974 [==============================] - 0s 39us/step - loss: 0.6931 - acc: 0.4945 - val_loss: 0.6931 - val_acc: 0.5050
Test loss: 0.6931471824645996
Test accuracy: 0.5050301810865191










